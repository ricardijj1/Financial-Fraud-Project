{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report \n",
    "1. Which insights did you gain from your EDA?\n",
    "From my EDA, I noticed some clear patterns. The most obvious one was that fraud mostly happened in CASH_OUT and TRANSFER transactions. I also saw that in many fraud cases, the origin account ended with a zero balance, which shows that accounts were being drained. Most of the balance-related features were right-skewed, with lots of values at or near zero. I had the idea to look into mismatched balances between old and new balances, thinking that might signal fraud — but it led nowhere. None of those mismatched transactions were actually fraud. I still decided to keep the new balance_difference column in my cleaned data for further testing.\n",
    "\n",
    "2. How did you determine which columns to drop or keep?\n",
    "I dropped the NameOrig and NameDest columns because they were just account IDs and didn’t help in finding any patterns. Most of the useful features came from the amount and balance columns, so I focused on those. I also added a new column called balance_difference to explore if fraud was linked to balance mismatches. Even though it didn’t directly connect to fraud, I kept it just in case it helps later in modeling.\n",
    "\n",
    "3. Which hyperparameter tuning strategy did you use? Grid-search or random-search? Why?\n",
    "I didn’t get to hyperparameter tuning yet. My plan was to try SMOTE for my Logistic Regression model to improve recall, but I ran into issues importing SMOTE from imblearn in my conda environment — it just wouldn’t load. I still want to get back to this and try out grid search once I have that fixed.\n",
    "\n",
    "4. How did your model's performance change after discovering optimal hyperparameters?\n",
    "I haven’t tuned the hyperparameters yet, so I didn’t see a change yet. But I’m hoping to try SMOTE and grid search in the future to see if that improves recall and F1 score — especially for fraud detection where recall matters most.\n",
    "\n",
    "5. What was your final F1 Score?\n",
    "My final F1 score for Logistic Regression was 0.58, and for my KNN model, it was 0.70. KNN ended up performing better in terms of catching fraud based on those metrics.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
